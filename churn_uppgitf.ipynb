{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a419c2ee",
   "metadata": {},
   "source": [
    "# Introduction!\n",
    "We are working with **a very obalance dataset**, and the task is to detect clients who **churn**. We have to choose a model that can predict  accurately those churns client to understand why they left the company, and to achieve that task i used the following models:<br>\n",
    "\n",
    "1) Logistic regression\n",
    "2) GradientBoostingClassifier\n",
    "3) RandomForest\n",
    "\n",
    "### **For the first 2 models** i applied the same methodology:\n",
    "After splitting the data into train e test, i trained the models and check the metrics. then i applied **a threshold** and check if the metrics got better. <br>Then, i used **SMOTE oversampling tecnique**, trained again those 2 model with the resempled data, and check the parameters  to see if the metrics could get even better. Finally i added a threshold even i the resempled data and check for the last time if the matrics got better.\n",
    "\n",
    "For **RandomForest** model instead, i didnt use thresholds or Smote, but i relied on an **inbuild parameter** called **class_weight='balanced'** that add **\"more weigth\"** to the minority class, so the model wont **\"ignore it\"**  \n",
    "\n",
    "I also encoded the data with labelEncoder and OneHotEncoder, and did some **feature selection**, deleting some features that wasn't relevant for the models.\n",
    "\n",
    "\n",
    "## What is important to think in this case?\n",
    "\n",
    "The goal is to get more **TP** as we can, at the lowest **FP** price. **TP** are the class 1 churn clients, and represent the clients that left the company. (A little reminder: When **TP** increase, **FN** decrease accordly.)  \n",
    "\n",
    "# A litle knowledge about the matrics that we going to check to ensure that the model is good for our purpuse:\n",
    "\n",
    "ðŸ”¹ **precision_score**: It measures in **%** how many of the predicted positive cases are actually correct compared to y_test(our real churn label 1). It answers: \"When the model predicts positive, how often is it right?\" High precision means lower false positives. \n",
    "\n",
    "ðŸ”¹ **Recall_Score**:\n",
    "Recall evaluates how well the model captures all actual positive cases. It answers: \"Of all real positives, how many did the model find?\" High recall means low false negatives.\n",
    "\n",
    "ðŸ”¹ **F1 Score**\n",
    "The F1 Score is the **harmonic mean of precision_score and recall_score**. It balances both metrics, especially useful in imbalanced datasets where high precision or recall alone may be misleading.\n",
    "\n",
    "# At the end of this page i ll show wich model i think is better for this task and why"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b583a307",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c787f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing the dataset\n",
    "# I dati del nostro dataset excel sono separati da \";\", quindi per evitale letture scorrete dei dati da parte di pandas, usiamo le flag sep=\";\" e decimal=\",\" \n",
    "df = pd.read_csv(\"elevera_data_set.csv\",sep=\";\" ,decimal=\",\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391e7205",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17dc8e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape # nr of rows and columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb0f9b2",
   "metadata": {},
   "source": [
    "### Delete the \"arbitrary\" data column \"customerID\", because \"It is an identifier\" and therefore the model cannot generalize well on the data, since customerID is a column whose values â€‹â€‹are repeated and therefore does not provide useful information to the classification model (indeed it can lead it to run into problems of poor generalization and overfitting)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad60cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df.drop(columns=[\"customerID\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907f710d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4deb90eb",
   "metadata": {},
   "source": [
    "### Let's see if any columns have missing data, and if so let's fill them with the average of the data in that column (for numeric features only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86eedd61",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum() # TotalCharges har 11 missinValues(nullValue). Nu vi ersÃ¤tter de values med the median of column values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ae388d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"TotalCharges\"].fillna(df[\"TotalCharges\"].mean(),inplace=True) #changed the  missingValues with the mean of the values of that column. \n",
    "# Ps There are other fillna techniques if we want to try in future to see if the model works better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd147ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum() # Data added correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80408aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0161362",
   "metadata": {},
   "source": [
    "###  As we know, models must be feed with numeric data to work, so the next step is to fund the categorical data in our columns, and trasform it with labelEncode, and onHoteEncoder.\n",
    "While classification models like xgBoost and randomForest can handle a simple LabelEncoder, linear or logistic regression models can lead to misclassification with columns with more than 3 different categories. So we transform columns with binary categories with a simple labesEncoded, and columns with more than 3 categories with OneHotEncoder.<br>\n",
    "**! important**. If some columns have tens or hundreds of categories, the number of new columns **will be very high**, and **you may run into memory or sparse data issues**.\n",
    "In the case of this dataset, however, this is not a problem because we only have 10 columns that contains a maximum of 4 different categories, so we can use this technique without any problems\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<!-- \n",
    "Anche se modelli di classificazioni come xgBoost e randomForest se la cavano bene anche con un semplice LabelEncoder, modelli lineari o di regressione logistica possono portare ad errori di classificazione con colonne con piu' di 3 categorie diverse. quindi trasformiamo le colonne con categorie binari con semplice labesEncoded, e le colonne con piu' di 3 categorie con OneHotEncoder.<br>\n",
    "**! importante**. Se alcune colonne hanno decine o centinaia di categorie, il numero di nuove colonne **sarÃ  molto alto**, e **potresti incorrere in problemi di memoria o sparse data**.\n",
    "Nel caso di questo db pero' non e' un problema perche' abbiamo solo 10 colonne che hanno al massimo 4 categorie diverse, quindi possiamo usare senza problemi questa tecnica -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ae0100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import encoders\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "le = LabelEncoder()\n",
    "Ohe = OneHotEncoder(drop='first', sparse_output=False) # drop='first' to avoid ridondant data, sparse_output='false' return a 2d array(good for Pandas)\n",
    "# Apply labelEncode and oneHotEncode accordly: (Uncomment the prints see the categories number)\n",
    "for col in df.select_dtypes(include=['object', 'category']).columns: # find categorical data\n",
    "    num_unique = df[col].nunique()\n",
    "    if num_unique > 2:\n",
    "        # Update the df columns with the encoded data\n",
    "         encoded = Ohe.fit_transform(df[[col]]) #  fit_Transform Only on train data,  just transform on test data. [[col]] => duble squarebracket to mantain the 2d array form\n",
    "         encoded_df = pd.DataFrame(encoded, columns=Ohe.get_feature_names_out([col])) # create a new dataset that contains the column with more than 2 cat data, and the new column name are same as the categorical data name\n",
    "         df = df.drop(columns=[col]) # drop the column with more than 2 categ data\n",
    "         df = pd.concat([df, encoded_df], axis=1) # add to our mother df the new oneHot encoded data\n",
    "         print(f\"Column '{col}' has {num_unique} categories\") #Uncomment to see Column name with more than 2 categorical data\n",
    "       \n",
    "    else:\n",
    "        print(f\"Column '{col}' has {num_unique} categories\")  #Uncomment to see Column name with  2 categorical data\n",
    "        df[col] = le.fit_transform(df[col])  # Only fit_Transform on train data, end just transform on test data\n",
    "        # Update the df columns with the encoded data\n",
    "\n",
    "## this loop check if the column has more or less than 2 categorical data, and use the encoder accordly "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632b63e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d7460d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522bea3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857f7c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['SeniorCitizen']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cefd6a5",
   "metadata": {},
   "source": [
    "# Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28fe44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['SeniorCitizen'].value_counts() # check if value count has to be dropped. (it has not to be)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13a971a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check ridondant data to drop (not found)\n",
    "constant_columns = [col for col in df.columns if df[col].nunique() <= 1 ]\n",
    "\n",
    "print(\"Colonne con un solo valore:\")\n",
    "print(constant_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f190ec33",
   "metadata": {},
   "source": [
    "### Down we check the correlations(positive or negative) between the columns in the dataset. The hight positive or negative related columns had to be dropp because they are not good for the model. The rows of each feature is translated as a **vector in the space**, that has n-rows dimentions. if in two or more column, all rows of the dataset has almost same values, its is like we have 2 vector that have the same direction in the space, and it confuse the model. And it the same if the correlation is negative, meaning that the vectore of one feature is the equivalent negative vector of anothe feature vector\n",
    "\n",
    "\n",
    "### PS, if oneHotEncoded data has ben done with the drop=\"first\" flag like i did, and labelEncoded has been used to trasform only those column with binary class, they will not have a correlation higher than 50%, so this encoded data will not be showed in the code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc37d810",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.8\n",
    "corr_matrix = df.corr(numeric_only=True)\n",
    "\n",
    "# Maschera diagonale e valori sotto soglia\n",
    "high_corr = corr_matrix[(abs(corr_matrix) > threshold) & (abs(corr_matrix) < 1.0)]\n",
    "print(high_corr.dropna(how='all').dropna(axis=1, how='all'))\n",
    "\n",
    "# As we see, the correlation between PhoneService and \"MultipleLines_No phone service\" has a perfect negative corr, meaning that MultipleLines_No phone service\n",
    "# has to be dropped(we dropp it because it is the more logical column to drop,and because is of course the less important feature betwen the 3. keep reading to understand more). \n",
    "# In fact, if we check the initial dataset before the encoding, we can notice that in the first 5 rows of df.head(), on every row\n",
    "# under PhoneService feature that has the values = \"NO\",  the value of MultipleLines feature is always equal to \"No phone service\". \n",
    "# So even if we drop that column \"MultipleLines_No phone service\", (that remember has been dropped by default with oneHotEncoding(drop=\"first\")), it wont be a problem\n",
    "# because PhoneService and MultipleLines_Yes alone will still represent the column values 'MultipleLines_No'. Thats because when 'PhoneService' value\n",
    "# is = 0(NO) and 'MultipleLines_Yes' values is = 0(NO), thats is equal to \"No phone service ('MultipleLines_No')\" value, so there is no need to keep MultipleLines_No, \n",
    "# because the model understand it by 'PhoneService' and 'MultipleLines_Yes' data. This way the dataset ll have less unecessary columns while still mantains all\n",
    "# important data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0fd23f",
   "metadata": {},
   "source": [
    "As we see in the output above, the correlation between PhoneService and \"MultipleLines_No phone service\" has a perfect negative corr, meaning that MultipleLines_No phone service has to be dropped(we dropp it because it is the more logical column to drop,and because is of course the less important feature betwen the 3. keep reading to understand more). <br>\n",
    "In fact, if we check the initial dataset before the encoding, we can notice that in the first 5 rows of df.head(), on every row\n",
    "under PhoneService feature that has the values = \"NO\",  the value of MultipleLines feature is always equal to \"No phone service\". \n",
    "So even if we drop that column \"MultipleLines_No phone service\", (that remember has been dropped by default with oneHotEncoding(drop=\"first\")), it wont be a problem\n",
    "because PhoneService and MultipleLines_Yes alone will still represent the column values 'MultipleLines_No'. Thats because when 'PhoneService' value is = 0(NO) and 'MultipleLines_Yes' values is = 0(NO), thats is equal to \"No phone service ('MultipleLines_No')\" value, so there is no need to keep MultipleLines_No, because the model understand it by 'PhoneService' and 'MultipleLines_Yes' data. This way the dataset ll have less unecessary columns while still mantains all important data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b39393",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2268dc0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets drop MultipleLines_No phone service  \n",
    "\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9cd0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=[\"MultipleLines_No phone service\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de06fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns # related column MultipleLines_No phone service dropped"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6217f9c1",
   "metadata": {},
   "source": [
    "### Usualy, features that contain >90/95% of constant values has to be dropped because that data is not relevant for the model. But in a obalanced Dataset like this one, if those columns are releted to with the target feature **(Churn)**, whe don drop it/those, because it means that those features contains important and relevant data for the prediction of churn. <br> Down i check all the features that has obalanced values, and then i confront those columns with the target value to see if there is important relation betwen them. if there is we dont delete the feature, otherwise we con delete that constsnt feature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2276ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets found all the obalanced features (least > 90% obalancing)\n",
    "for col in df.columns:\n",
    "    top_freq = df[col].value_counts(normalize=True).values[0] # values[0] is equal to the most dominant value in the features  \n",
    "    if top_freq > 0.9:\n",
    "        print(f\"{col}:{top_freq:.2%} same values\") # .2% = the decimal nr to show (could be 3,4...)\n",
    "\n",
    "\n",
    "# here we got the features name that are very obalanced: Churn of course, and PhoneService\n",
    "# PS be sure that the columns have no missing values to perform this operation (already done with df.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2ac69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# analize those 2 columns\n",
    "df[\"PhoneService\"].value_counts(normalize=True) \n",
    "# 1(yes)  =  0.901563 => (.values[0], the dominant value)\n",
    "# 0(no)  =  0.098437\n",
    "# 90.1% has phone\n",
    "# 9.8% has not the phone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90843e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Churn\"].value_counts(normalize=True)\n",
    "# 0(no)  =  0.962784 => (.values[0], the dominant value)\n",
    "# 1(yes) =   0.037216\n",
    "# 96% clients stayed with the company\n",
    "# 4% left the company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5dd261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.crosstab shows a table that confront all values of Feature column with the value of Churn column.\n",
    "# Thats means that in all rows where PhoneService values is 0(No), 0.967864(96%) of values in Churn has 0(No) too, and 0.032136(3%) values 1(yes)\n",
    "# with the same logic, in all rows where PhoneService values is 1(yes), 0..962229(96%) of values in Churn has 0(No) and 0.037(3%) has value 1(yes)\n",
    "pd.crosstab(df['PhoneService'], df['Churn'], normalize='index')\n",
    "# its kinda do a median where the class are the same row for row, and return the procent of the maching values for that value (0 and 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63765e5",
   "metadata": {},
   "source": [
    "### This tabel is very usefull to understand if a feature is associated to target Churn and wich category has more probability of \"Churn\".<br><br>\n",
    "\n",
    "### What does this table mean?<br> Even though most customers have PhoneService = 1, the probability of churn is slightly higher in that group (3.8% vs. 3.2%).<br><br>This small difference may contain information that helps the model distinguish churners from non-churners. in simple word that means that the Clas 1(Yes) in PhoneService, is relevant for the churn class 1(yes) and thats why we don delete it for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e5e1cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9183d8b8",
   "metadata": {},
   "source": [
    "# Now lets split the dataset with trai_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826d38f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets move the churn column at the last column position, because right now it is at position 9:\n",
    "cols = list(df.columns)\n",
    "col_to_move = cols[9]\n",
    "cols.append(cols.pop(9))\n",
    "df = df[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3748d1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:,:-1].values # 2D array\n",
    "y = df.iloc[:,-1].values # 1d array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7095fcf",
   "metadata": {},
   "source": [
    "## Data split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e11019e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3eca47",
   "metadata": {},
   "source": [
    "### Before scaling the data, let's see how many outliers there are in our dataset. The code below divides the dataset into percentiles, calculates the outliers in each feature, and calculates the total average of the outliers of all the columns. <br> If this final average is higher than > 5%, then we should use **RobustScaler**, which scales the data by calculating the median and IQR(x) so that the outliers do not affect the scaling of the \"good\" data (centered around the mean), but are only reflected in the scale of the **outliers themselves**. This scaling method is recommended when there are many outliers in the dataset precisely because they do not affect the scaling of the coherent data, i.e. those that do not distance themselves from the mean (\"no-outliers\"). <br>\n",
    "\n",
    "### If instead the code shows us that there are relatively few total outliers, then we could use the classic **StandardScaler**\n",
    "\n",
    "<!-- ###  Prima di scalare i dati, vediamo quandi outliers ci sono nel nostro dataset. Il codice in basso divide il dataset in percentile, calcola gli outliers in ognuna feature, e fa la media totale degli outliers di tutte le colonne. <br> Se questa la media finale e superiore > 5%, allora ci conviene usare **RobustScaler**, che scala i dati calcolando la mediana e IQR(x) in modo che gli outliers non influenzino la scalature dei dati \"buoni\"(incentrati intorno alla media), ma si riflettono solo sulla scala degli **outliers stessi**. Questo metodo di scalatura e' consigliato in presenza di tanti outliers nel dataset prorio perche' questi non influenzano la scalatura dei dati coerenti, cioe' quelli  che non si distanziano dalla media(\"no-outliers\"). <br>\n",
    "\n",
    "### Se invece il codice ci mostra che ci sono relativamente pochi outliers totali, allora potremmo usare il sclassico **StandardScaler**. -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a6b5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def check_outliers_iqr(X):\n",
    "    n_features = X.shape[1]\n",
    "    outlier_percentages = []\n",
    "\n",
    "    for i in range(n_features):\n",
    "        col = X[:, i]\n",
    "        Q1 = np.percentile(col, 25)\n",
    "        Q3 = np.percentile(col, 75)\n",
    "        IQR = Q3 - Q1\n",
    "\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "        outliers = np.sum((col < lower_bound) | (col > upper_bound))\n",
    "        percent_outliers = (outliers / len(col)) * 100\n",
    "        outlier_percentages.append(percent_outliers)\n",
    "        print(f\"Feature {i}: {percent_outliers:.2f}% of outlier\")\n",
    "\n",
    "    # Media % outlier su tutte le feature\n",
    "    mean_outliers = np.mean(outlier_percentages)\n",
    "    print(f\"\\nAverage outlier on all the feature: {mean_outliers:.2f}%\")\n",
    "\n",
    "    # Suggerimento semplice sullo scaler\n",
    "    if mean_outliers > 5:\n",
    "        print(\"Ther are pretty much outlier, consider using RobustScaler.\")\n",
    "    else:\n",
    "        print(\"Ther are pretty low outlier, You can use StandardScaler.\")\n",
    "\n",
    "    return outlier_percentages\n",
    "\n",
    "# Uso\n",
    "outlier_perc = check_outliers_iqr(X_train)\n",
    "\n",
    "# We can use StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb388a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets scale the data with StandardScaler:\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sc_X = StandardScaler()\n",
    "X_train = sc_X.fit_transform(X_train) # fit_transform only for train data\n",
    "X_test = sc_X.transform(X_test)  # trasform for test data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ba16b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46248de1",
   "metadata": {},
   "source": [
    "### Trainin a Logistic regression model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36aa7790",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "sigm_classifier = LogisticRegression(random_state=0)\n",
    "\n",
    "sigm_classifier.fit(X_train,y_train) # train the model with train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8eba7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = sigm_classifier.predict(X_test) # lets do the test with our scaled test data and check if the metrics are good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a152d2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740ccdd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709541b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "ac = accuracy_score(y_test, y_pred) ## con le metriche si misura sempre i dati reali di y_test e i dati delle predizioni di X_test\n",
    "cm = confusion_matrix(y_test,y_pred)\n",
    "ps = precision_score(y_test,y_pred)\n",
    "rec = recall_score(y_test,y_pred)\n",
    "f1_s = f1_score(y_test,y_pred)\n",
    "auc= roc_auc_score(y_test,y_pred)\n",
    "# password supabase sql db = JYpDBngOoUhg2sUW\n",
    "# https://supabase.com/dashboard/project/myrqslnfopwsqibvevrn/auth/policies?search=17269&schema=public\n",
    "print('accuracy_score = ', ac)\n",
    "print('confusion_matrix:')\n",
    "print(cm)\n",
    "print(' ')\n",
    "print('precision_score = ', ps)\n",
    "print('recall_score = ', rec)\n",
    "print('f1_score = ', f1_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8f2dbc",
   "metadata": {},
   "source": [
    "### Modellen visade sig inte vara bra att classificera classen 1 i charn feature. confusion_matrix visar att modellen classificerade allt som 0(no), och att det missade 38 TP values som Ã¤r det mest viktiga data att predicera  <br> Det beteende Ã¤r vanligt nÃ¤r man jobbar med obalanserad dataset. FÃ¶r att Ã¶ka antal TP , vi ska testa att sÃ¤nka modellen's decision threshold:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d4bdf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function that on call will return auc value of each different models\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "def calculate_models_auc(models_props):\n",
    "    fpr,tpr,thresholds = roc_curve(y_test,models_props)\n",
    "    roc_auc = auc(fpr,tpr)\n",
    "    return roc_auc\n",
    "\n",
    "# in y_test there are:\n",
    "# 38 1(yes) values\n",
    "# 1037 0(no) values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1488e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_probs = sigm_classifier.predict_proba(X_test)[:,1] # get the result of probability only of churn 1\n",
    " #(y_pred >=threshold).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b3805d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff7a845",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lower threshold\n",
    "threshold = 0.3\n",
    "y_pred_custom = (y_probs >= threshold).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26226cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_custom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e3e4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the metrics \n",
    "print('confusion_matrix:\\n', confusion_matrix(y_test, y_pred_custom))\n",
    "print('classification_report:\\n', classification_report(y_test, y_pred_custom))\n",
    "print(\"Roc =\", calculate_models_auc(y_probs))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0eca5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr,tpr,thresholds = roc_curve(y_test,y_probs)\n",
    "roc_auc = auc(fpr,tpr)\n",
    "plt.plot(fpr, tpr, color='b', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
    "\n",
    "# Aggiungi la diagonale (linea casuale)\n",
    "plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
    "\n",
    "# Aggiungi etichette e titolo\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Curva ROC')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "# Mostra il grafico\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc95a1d5",
   "metadata": {},
   "source": [
    "### As we can see in the confusion matrics, even if we changed the decision threshold the model still not precise at all to classify the minority class. \n",
    "**!important fact**: the values â€‹â€‹of **roc_curve and auc** seems good even though the model fails to separate the classes efficiently. this happens when we apply these metrics on **unbalanced datasets like this one**, because these metrics measure the overall ranking ability, but it does not take into account that the actual performance **on minority** classes can be poor. So it is not worth using them metrics in this context\n",
    "\n",
    "\n",
    "\n",
    "# Lets Try with **SMOTE** tecnique to see if we can find a perfect balance, and achieve lower FP and **even more important lower FN**\n",
    "\n",
    "### What is SMOTE? <br>\n",
    "\n",
    "It is a oversempling tecnique that create sintetic data rows of the indipendent variables that has the minority class churn. It create similar data, **not copy** of the indipendent variables, and that why it is better than traditional **random oversampling**.<br>\n",
    "Bacause use **K-NN** to create similar data, **its very important that data has been scaled with Standard or robustScaler**.\n",
    "This metod will add extra data to the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6331c9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE # !pip install imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae68a4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "smote = SMOTE(sampling_strategy='auto', random_state=0, k_neighbors=5)\n",
    " # sampling_strategy='auto',  samples only the minority class\n",
    "# k_neighbors=3 (generate syntetic data from the 5 nearest neighbors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88eaa3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape # total X_train data before smothe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a9e103",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f343f090",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_resampled.shape # SMOTE  added 4000 more syntetic data that is similar to the data belonging to the minority churn class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b1cbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_resampled.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d5ff70",
   "metadata": {},
   "source": [
    "### Now SMOTE turned the dataset to a **balanced one**. Lets train the same model again with SMOTE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28336ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if smote added correctly the datas. churn classes should be 50-50 now\n",
    "print(pd.Series(y_train_resampled).value_counts())\n",
    "# Now the dataset is balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c364b13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model again:\n",
    "SMOTE_sigm_classifier = LogisticRegression(random_state=0)\n",
    "SMOTE_sigm_classifier.fit(X_train_resampled,y_train_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe8019f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict data:\n",
    "\n",
    "smote_y_pred = SMOTE_sigm_classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea088ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check metrics:\n",
    "print(\"Confusion matrics =\\n\", confusion_matrix(y_test,smote_y_pred))\n",
    "print(classification_report(y_test,smote_y_pred))\n",
    "# print(precision_score(y_test,smote_y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c790da",
   "metadata": {},
   "source": [
    "### With SMOOTE, the TP values raised, meaning that the model can recognize better the minority class 1. In fact, the FN are also lowered, and thats is also important because its meaning that the model dont classify as 0 the class that are actually 1. However, FP (0 real values that the model classified as 1) still hight.<br> \n",
    "\n",
    "# We dont need to consider the Precision Matrix, because in a obalanced dataset this matrix is missleading. \n",
    "## The recall matrics looks pretty accurate because the model predicted correctly 76% of the minority class 1 \n",
    "# F1 score is a looks (continue later)\n",
    "\n",
    "# Lets lower the threshol to see if we can get some more TP, possibly at a low FP cost\n",
    "<!-- Now lets if lowering the threshold here can get even better  -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b3927d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "30b35c73",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94342347",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lower the threshold\n",
    "\n",
    "SMOTE_y_probs = SMOTE_sigm_classifier.predict_proba(X_test)[:,1] \n",
    "SMOTE_threshold = 0.3\n",
    "SMOTE_y_pred_custom = (SMOTE_y_probs >= SMOTE_threshold).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6182c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Confusion matrics =\\n\", confusion_matrix(y_test,SMOTE_y_pred_custom))\n",
    "print('classification_report:\\n', classification_report(y_test, SMOTE_y_pred_custom))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd5c533",
   "metadata": {},
   "source": [
    "### We can consider to lower even more the threshold if we really need a few more TP, but we gotta be aware that it will **cost many FP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b554de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f7c1e9ad",
   "metadata": {},
   "source": [
    "# Now lets see if we can do better  with the powerful **GradientBoostingClassificator**, which is one of the most powerful and **recommended** classifiers specially when dealing with unbalanced datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4748c590",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the classifier:\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1eace0",
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm = GradientBoostingClassifier(n_estimators=100,learning_rate=0.1, max_depth=3,random_state=0)\n",
    "# n_estimators=100 = number of trees \n",
    "# learning_rate Check how much each tree contributes to the final prediction. (Between 0.1 is good. higher can lead to overfitting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179555ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a35844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets do the predictions on X_test as usually:\n",
    "\n",
    "gbm_y_pred = gbm.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b269f3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm_y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87aacbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Confusion Matrix  \\n',confusion_matrix(y_test, gbm_y_pred))\n",
    "print('classification_report:\\n', classification_report(y_test, gbm_y_pred))\n",
    "\n",
    "# By default, GradientBoostingClassifier is sligtly better than the logistic regressor classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7551d69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "83636182",
   "metadata": {},
   "source": [
    "<!-- ### Train  GradientBoostingClassifier with SMOTE data -->\n",
    "### lower the threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff55403",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gbm_threshold = 0.3\n",
    "# Lower the threshold\n",
    "\n",
    "gbm_y_probs = gbm.predict_proba(X_test)[:,1] # [:,1] shows the probability  that the model assign the class 1 based on the data of each row in X_test \n",
    "gbm_threshold = 0.2\n",
    "gbm_y_pred_custom = (gbm_y_probs >= gbm_threshold).astype(int) # astype(int) make False True to 0 and 1, so it make a new gbm_y_probs like array with lowered thresold data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fd0456",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gbm.predict_proba(X_test)) #[0.98334267 0.01665733]] 0.98334267 = prob that model assign class 0 at this row. \n",
    "# 0.01665733 prob that model assign class 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6837bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check confusion matrics and other matrics:\n",
    "# gbm_y_pred_custom\n",
    "print(f'Confusion Matrix  \\n',confusion_matrix(y_test, gbm_y_pred_custom))  # always y_test at the begin of the matrics, and then\n",
    "print('classification_report:\\n', classification_report(y_test, gbm_y_pred_custom))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936861a4",
   "metadata": {},
   "source": [
    "### By lowering the thresold to 0.2 gained some  TP, but at a cost of pretty more FP. <br><br>Lets create a function that will find the threshold that has the best f1_score: \n",
    "(ps. With the same logic of this function we can also find the threshold with the best recall, or other matrics we wish. we can also add constrain, to find the best recal with the condition that precision_score has to be = 80%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c92281",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_threshold = 0.5\n",
    "best_f1 = 0\n",
    "metrics = []\n",
    "\n",
    "thresholds = np.arange(0.0, 1.01, 0.01)\n",
    "\n",
    "for t in thresholds:\n",
    "    y_pred = (gbm_y_probs >= t).astype(int)\n",
    "\n",
    "    precision = precision_score(y_test, y_pred, zero_division=0)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    metrics.append((t, precision, recall, f1))\n",
    "    \n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_threshold = t\n",
    "\n",
    "print(f\"âœ… Best threshold for F1: {best_threshold:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489e5ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign the best threshold:\n",
    "gbm_threshold = 0.18\n",
    "gbm_y_pred_custom = (gbm_y_probs >= gbm_threshold).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56bdb7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Confusion Matrix  \\n',confusion_matrix(y_test, gbm_y_pred_custom))  # always y_test at the begin of the matrics, and then\n",
    "print('classification_report:\\n', classification_report(y_test, gbm_y_pred_custom))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3872ded2",
   "metadata": {},
   "source": [
    "### Now, lets train this classifier with SMOTE DATA and see what hapens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d9246e",
   "metadata": {},
   "outputs": [],
   "source": [
    "SMOTE_gbm = GradientBoostingClassifier(n_estimators=100,learning_rate=0.1, max_depth=3,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d68ee3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "SMOTE_gbm.fit(X_train_resampled,y_train_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c73bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "smote_gbm_y_pred = SMOTE_gbm.predict(X_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6eb578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the metrics:\n",
    "print(f'Confusion Matrix  \\n',confusion_matrix(y_test, smote_gbm_y_pred)) \n",
    "print('classification_report:\\n', classification_report(y_test, smote_gbm_y_pred))\n",
    "\n",
    "# As we can see the TP incresed and the FN decreased in confront to the lowered threshold. however the FP also increased and the TN decreased a bit\n",
    "# But it detect better the minority class. lets lower the thres on this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ade5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "Smote_gbm_y_probs = SMOTE_gbm.predict_proba(X_test)[:,1] # [:,1] shows the probability  that the model assign the class 1 based on the data of each row in X_test \n",
    "smothe_gbm_threshold = 0.20\n",
    "Smote_gbm_y_pred_custom = (Smote_gbm_y_probs >= smothe_gbm_threshold).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac8af30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check matrics conf matrict on GradientBostingClassifier trained with SMOTE data\n",
    "print(f'Confusion Matrix  \\n',confusion_matrix(y_test, Smote_gbm_y_pred_custom)) # always y_test in the matrics\n",
    "print('classification_report:\\n', classification_report(y_test, Smote_gbm_y_pred_custom))\n",
    "\n",
    "# we dubbled the TP but tripled th FP, meaning that many TN has not classified correctly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faac31cd",
   "metadata": {},
   "source": [
    "### Now lets change Classifier, and lets use another tecnique beside Thresholds and SMOTE to balance the dataset. This is and inbuild function in many ensamble classifier, called Class_weight='balanced'. It (continua dopo a spiegare come funziona classweight)\n",
    "# No need to add syntetic data with this metod, good if we have a moderate dataset (not too big)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17205bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caeae92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "RF = RandomForestClassifier(class_weight='balanced',\n",
    "                            random_state=0,\n",
    "                            n_estimators=250,\n",
    "                            min_samples_split=20,       \n",
    "                            max_depth=8 )  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9324ce82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "\n",
    "RF.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8abc611f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict data:\n",
    "\n",
    "RF_y_pred = RF.predict(X_test) # predict X_test =  y_pred data, that then has to be confronted with real y_test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597527da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the metrics:\n",
    "\n",
    "print(f'Confusion Matrix  \\n',confusion_matrix(y_test, RF_y_pred))\n",
    "print('classification_report:\\n', classification_report(y_test, RF_y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db40175",
   "metadata": {},
   "source": [
    "### The function below help us find the best RandomForestClassifier parameters that will get the best recall:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38013717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets try if we can have a better recall matric using GridSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [5, 10, None],\n",
    "    'min_samples_split': [2, 10, 20],\n",
    "    'class_weight': ['balanced']\n",
    "} # here we describes all the parameters that we want to test to find the best recall \n",
    "\n",
    "rf2 = RandomForestClassifier(random_state=0)\n",
    "\n",
    "grid_search = GridSearchCV(estimator=rf2, param_grid=param_grid,\n",
    "                           scoring='recall', # the metrics we are interested\n",
    "                           cv=5, n_jobs=1, verbose=1\n",
    "                           ) # here we test the model with those params\n",
    "\n",
    "grid_search.fit(X_train,y_train)\n",
    "\n",
    "print(\"Best params:\", grid_search.best_params_)\n",
    "print(\"Best score:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7581c355",
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_recall = RandomForestClassifier(class_weight='balanced',\n",
    "                            random_state=0,\n",
    "                            n_estimators=100,\n",
    "                            min_samples_split=20,       \n",
    "                            max_depth=5\n",
    "                                   )\n",
    "RF_recall.fit(X_train,y_train)\n",
    "# We got more TP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a04d052",
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_recall_y_pred = RF_recall.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067668c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the metrics:\n",
    "print(f'Confusion Matrix  \\n',confusion_matrix(y_test, RF_recall_y_pred))\n",
    "print('classification_report:\\n', classification_report(y_test, RF_recall_y_pred))\n",
    "# As wee see, we incresed the recall(TP) thats grate, but it costed 100 FP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12d89f3",
   "metadata": {},
   "source": [
    "# Lets sample all the metrics:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f679da00",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1c57d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(' Sigmoid Threshold confusion_matrix:\\n', confusion_matrix(y_test, y_pred_custom), \"\\n\")\n",
    "print( \"Sigmoid Threshold Classification report:\\n\", classification_report(y_test, y_pred_custom), \"\\n\")\n",
    "print(\" Sigmoid_Smote Confusion matrics =\\n\", confusion_matrix(y_test,smote_y_pred), \"\\n\")\n",
    "print( \"Sigmoid_Smote Classification report:\\n\" ,classification_report(y_test,smote_y_pred), \"\\n\")\n",
    "print(\" Sigmoid_Smote Threshold Confusion matrics =\\n\", confusion_matrix(y_test,SMOTE_y_pred_custom), \"\\n\")\n",
    "print(\"Sigmoid_Smote Threshold classification report= \\n\", classification_report(y_test,SMOTE_y_pred_custom), \"\\n\")\n",
    "print(f'GradientBostingClassifier Confusion Matrix  \\n',confusion_matrix(y_test, gbm_y_pred), \"\\n\")\n",
    "print(\"GradientBostingClassifier classification Report \\n =\",classification_report(y_test,gbm_y_pred), \"\\n\")\n",
    "print(f'GradientBostingClassifier Threshold Confusion Matrix  \\n',confusion_matrix(y_test, gbm_y_pred_custom), \"\\n\")\n",
    "print(\"GradientBostingClassifier Threshold classification_report \\n= \",classification_report(y_test,gbm_y_pred_custom), \"\\n\")\n",
    "print(f'GradientBostingClassifier Smote Confusion Matrix  \\n',confusion_matrix(y_test, smote_gbm_y_pred), \"\\n\") \n",
    "print(\"GradientBostingClassifier Smote classification_report \\n\", classification_report(y_test,smote_gbm_y_pred), \"\\n\")\n",
    "print(f' \"GradientBostingClassifier Smote Threshold Confusion Matrix  \\n',confusion_matrix(y_test, Smote_gbm_y_pred_custom), \"\\n\") \n",
    "print(\"GradientBostingClassifier Smote Threshold classification_report \\n\", classification_report(y_test,Smote_gbm_y_pred_custom), \"\\n\")\n",
    "print(f'RandomForest weight_balanced Confusion Matrix  \\n =',confusion_matrix(y_test, RF_y_pred), \"\\n\")\n",
    "print(\"RandomForest weight_balanced classification_report \\n = \",classification_report(y_test,RF_y_pred), \"\\n\")\n",
    "print(f' RandomForest best Recal Confusion Matrix   \\n',confusion_matrix(y_test, RF_recall_y_pred), \"\\n\")\n",
    "print(\"RandomForest bestRecal \\n =\",classification_report(y_test,RF_recall_y_pred), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767641ab",
   "metadata": {},
   "source": [
    "### Ja vÃ¥lde 2 modeller: <br>\n",
    "Sigmoid Modellen trÃ¤nad med resampled SMOTE data, och GradientBostingClassifier trÃ¤nad med resampled SMOTE data ach med sÃ¤nkt treshold 0.2. <br><br>\n",
    "Sigmoid_Smote:<br>\n",
    " [[805 232]<br>\n",
    " [  9  29]] \n",
    "\n",
    "Classification report:\n",
    "\n",
    "               precision    recall  f1-score   support\n",
    "\n",
    "           0       0.99      0.78      0.87      1037\n",
    "           1       0.11      0.76      0.19        38\n",
    "\n",
    "    accuracy                           0.78      1075\n",
    "   macro avg       0.55      0.77      0.53      1075\n",
    "weighted avg       0.96      0.78      0.85      1075\n",
    "\n",
    "<br>\n",
    "\n",
    "GradientBostingClassifier Smote Threshold:<br>\n",
    " [[903 134] <br>\n",
    " [ 16  22]] \n",
    "\n",
    "classification_report \n",
    "               precision    recall  f1-score   support\n",
    "\n",
    "           0       0.98      0.87      0.92      1037\n",
    "           1       0.14      0.58      0.23        38\n",
    "\n",
    "    accuracy                           0.86      1075\n",
    "   macro avg       0.56      0.72      0.58      1075\n",
    "weighted avg       0.95      0.86      0.90      1075\n",
    "\n",
    "<br>\n",
    "\n",
    "I chose these models because they have a good number of **TP**, and not too many **FP**. The **precision, recall and f1_score** metrics are better than the other models, and overall they are not bad given the significant imbalance of the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42607842",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef88b07b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
